{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "COLUMNS='ABCD'\n",
    "SIZE_100MiB = 100 * 1024 * 1024\n",
    "PARALLELISM = 8\n",
    "\n",
    "import tempfile\n",
    "from ray.data.datasource import RandomIntRowDatasource\n",
    "\n",
    "def generate_example_files(size_bytes: int) -> str:\n",
    "    tmpdir = tempfile.mkdtemp()\n",
    "    ray.data.read_datasource(\n",
    "        RandomIntRowDatasource(),\n",
    "        n=size_bytes // 8 // len(COLUMNS),\n",
    "        num_columns=len(COLUMNS)).write_parquet(tmpdir)\n",
    "    return tmpdir\n",
    "\n",
    "example_files_dir = generate_example_files(SIZE_100MiB)\n",
    "# Try `ls` on this directory from shell to see what files were created.\n",
    "print(example_files_dir)\n",
    "\n",
    "ds = ray.data.read_parquet(example_files_dir)\n",
    "ds.show(limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining execution with Ray DatasetPipelines\n",
    "\n",
    "Datasets execute their transformations synchronously in blocking calls. However, it can be useful to overlap dataset computations with output. This can be done with a DatasetPipeline.\n",
    "\n",
    "A [DatasetPipeline](https://docs.ray.io/en/master/data/dataset-pipeline.html) is an unified iterator over a (potentially infinite) sequence of Ray Datasets, each of which represents a window over the original data. Conceptually it is similar to a Spark DStream, but manages execution over a bounded amount of source data instead of an unbounded stream. Ray computes each dataset window on-demand and stitches their output together into a single logical data iterator. DatasetPipeline implements most of the same transformation and output methods as Datasets (e.g., map, filter, split, iter_rows, to_torch, etc.).\n",
    "\n",
    "This enables us to turn executions that look like this:\n",
    "![no pipeline](https://docs.ray.io/en/master/_images/dataset-pipeline-1.svg)\n",
    "\n",
    "Into something like this, where each block is a Dataset operation that can be pipelined with the other steps:\n",
    "![pipeline](https://docs.ray.io/en/master/_images/dataset-pipeline-2.svg)\n",
    "\n",
    "## Creating a DatasetPipeline\n",
    "\n",
    "A DatasetPipeline can be constructed in two ways: either by pipelining the execution of an existing Dataset (via Dataset.window), or generating repeats of an existing Dataset (via Dataset.repeat). Similar to Datasets, you can freely pass DatasetPipelines between Ray tasks, actors, and libraries.\n",
    "\n",
    "Let's get started by taking our Dataset from the previous example and transforming it into a DatasetPipeline. Try it using `ds.repeat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.dataset_pipeline import DatasetPipeline\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# This line returns immediately.\n",
    "# The DatasetPipeline execution happens lazily, as data gets cleared from the pipeline.\n",
    "pipe = ds.repeat(NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a DatasetPipeline, we can shuffle the data and pipeline that shuffle with some other execution, like a training task. This time, we'll use `.random_shuffle_each_window()` to shuffle the data in windows instead of shuffling the whole Dataset. The resulting execution will look something like this:\n",
    "\n",
    "![ingest](https://docs.ray.io/en/master/_images/dataset-repeat-1.svg)\n",
    "\n",
    "But what if we're using distributed training? Then, we actually need multiple synchronized pipelines, to make sure that each distributed training worker has a disjoint subset of the shuffled data. We can do this with `.split()`, which will then give us something like this:\n",
    "\n",
    "![e2e-ingest](https://docs.ray.io/en/master/_images/dataset-repeat-2.svg)\n",
    "\n",
    "Let's try this out with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(num_splits):\n",
    "    return ds.repeat(NUM_EPOCHS) \\\n",
    "            .random_shuffle_each_window() \\\n",
    "            .split(num_splits, equal=True)\n",
    "\n",
    "# What does this print? Why?\n",
    "splits = create_pipeline(3)\n",
    "for i, shard in enumerate(splits):\n",
    "    print(i, shard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming a distributed DatasetPipeline\n",
    "\n",
    "Now that we've created a distributed and shuffling data-loader, we can start consuming it!\n",
    "\n",
    "We'll do that with a pool of actors, each of which will take one of the DatasetPipeline shards and start reading batches. Each batch will be one shuffled window of the dataset, i.e. a group of rows, each with 4 columns.\n",
    "\n",
    "Let's define an actor that counts the number of rows that it's seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, rank: int, shard: DatasetPipeline):\n",
    "        self.rank = rank\n",
    "        self.shard = shard\n",
    "        \n",
    "        self.num_rows = 0\n",
    "\n",
    "    def train(self):\n",
    "        for epoch, training_dataset in enumerate(self.shard.iter_datasets()):\n",
    "            # Following code emulates epoch based SGD training.\n",
    "            print(f\"Training... worker: {self.rank}, epoch: {epoch}\")\n",
    "            for i, batch in enumerate(training_dataset.iter_batches(batch_format=\"pandas\")):\n",
    "                self.num_rows += len(batch)\n",
    "                # Can replace with actual training code.\n",
    "\n",
    "        return self.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put this all together, and run the pipeline!\n",
    "\n",
    "This will start 3 trainers and iterate over the full dataset for 3 iterations (epochs).\n",
    "Each trainer should see exactly 1/3 of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "NUM_TRAINERS = 3\n",
    "splits = create_pipeline(3)\n",
    "training_workers = [\n",
    "    TrainingWorker.remote(rank, shard) for rank, shard in enumerate(splits)\n",
    "]\n",
    "\n",
    "# Let's run the e2e pipeline\n",
    "start = time.time()\n",
    "print(ray.get([worker.train.remote() for worker in training_workers]))\n",
    "print(f\"total ingestion time: {int(time.time() - start)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
