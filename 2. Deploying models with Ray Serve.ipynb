{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying your first model\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/master/serve/) is a library for scalable and programmable model serving.\n",
    "It aims to address some of the major challenges found in model serving:\n",
    "\n",
    "- **Framework-agnostic:** Model serving frameworks must be able to serve models from popular systems like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks.\n",
    "- **Supports application logic:** Machine learning models are typically surrounded by lots of application logic. In our application, this will come up when we decide which types of movie recommendations to serve to a particular user based on information about what that user has selected before.\n",
    "- **Python-first:** Configure your model serving with pure Python code - no more YAML or JSON configs.\n",
    "- **Simple and scalable:** Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "- **Flexible deployment patterns:** Ray Serve makes it easy to deploy a forest of models and to split traffic to different instances.\n",
    "\n",
    "See this [blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) and the [docs](https://docs.ray.io/en/master/serve/) for more background on Ray Serve!\n",
    "\n",
    "In this notebook, we'll deploy our first models using Ray Serve.\n",
    "We'll deploy one model that serves movie recommendations based on the movie cover's color palette and another that serves movie recommendations based on the movie's plot.\n",
    "\n",
    "## If you didn't finish notebook 1:\n",
    "\n",
    "Run the next cell to finish filling out the missing movie palettes in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Fill out the missing movie palettes in the database, in case you haven't finished notebook 1.\n",
    "bash run_1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can connect to the cluster with `ray.init`.\n",
    "This is the same as in the previous notebook: we'll pass in the argument `address=auto` to indicate that we should connect to an existing cluster that is running on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(address=\"auto\", ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll start Serve. This will set up an empty Flask server that can serve HTTP requests.\n",
    "After this cell, we'll have something that looks like the diagram above, except without the \"Endpoint\" and \"Backend\" boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "\n",
    "try:\n",
    "    client = serve.start(detached=True)\n",
    "except:\n",
    "    # Skip if we already started Serve.\n",
    "    client = serve.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backends and endpoints\n",
    "\n",
    "Ray Serve has two key concepts, *backends* and *endpoints*.\n",
    "Backends define the implementation of your business logic or models that will handle requests, and *endpoints* define how user requests should be routed to the various backends.\n",
    "\n",
    "Each backend can have many replicas, which are individual processes running in the Ray cluster to handle requests.\n",
    "To define a backend, first you must define the “handler” or the business logic you’d like to respond with.\n",
    "The handler should take as input a Flask Request object and return any JSON-serializable object as output.\n",
    "The implementation can be defined as either a function or a class. Use a function when your response is stateless and a class when you might need to maintain some state (like a model).\n",
    "\n",
    "An endpoint is used to expose a backend to HTTP.\n",
    "Each endpoint can have one or multiple backends that serve requests; in our case, we'll use one backend per endpoint.\n",
    "\n",
    "In this notebook, we'll create one backend and endpoint each for the color-based recommender and the plot-based recommender.\n",
    "By the end of this notebook, we'll have a system that looks something like this:\n",
    "\n",
    "![](serve-notebook-2.jpg \"Ray Serve diagram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an endpoint\n",
    "\n",
    "First, we'll define a *backend* for the server.\n",
    "Our first backend will be a *stateful* class that serves movie recommendations based on a movie cover's color palette.\n",
    "Each request to the backend will include the ID of a movie that the user liked (`liked_id`).\n",
    "The backend will use k-nearest neighbors to determine the movies closest to the user's selected movie.\n",
    "Since it would be expensive to have to reload the index on each request, we'll use a *stateful* backend to keep the index in memory between requests.\n",
    "\n",
    "We'll specify the logic that should get run on the server in the `__call__` method.\n",
    "Serve will pass each Flask request that gets routed to this backend as an argument to this method.\n",
    "That way, we can access any user arguments that are passed in the request, such as parameters in an HTTP `GET` request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_db_connection, KNearestNeighborIndex\n",
    "\n",
    "\n",
    "class ColorRecommender:\n",
    "    def __init__(self):\n",
    "        self.db = get_db_connection()\n",
    "\n",
    "        # Create index of cover image colors.\n",
    "        colors = self.db.execute(\"SELECT id, palette_json FROM movies\")\n",
    "        self.color_index = KNearestNeighborIndex(colors)\n",
    "\n",
    "    def __call__(self, request):\n",
    "        liked_id = request.args[\"liked_id\"]\n",
    "        num_returns = int(request.args.get(\"count\", 6))\n",
    "\n",
    "        # Perform KNN search for similar images.\n",
    "        recommended_ids = self.color_index.search(liked_id, num_returns)\n",
    "\n",
    "        # Let's perform some post processing.\n",
    "        titles_and_ids = self.db.execute(\n",
    "            f\"SELECT title, id FROM movies WHERE id in ({','.join(recommended_ids)})\"\n",
    "        ).fetchall()\n",
    "\n",
    "        # Wrangle the data for JSON\n",
    "        return [{\n",
    "            \"id\": movie_id,\n",
    "            \"title\": title\n",
    "        } for title, movie_id in titles_and_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an instance of the backend and define an *endpoint* that exposes it to HTTP.\n",
    "This will tell Serve which traffic should go to the `ColorRecommender` instance.\n",
    "\n",
    "The `create_backend` call gives Serve a name for the backend (`\"color:v1\"`) and the class or function that contains the logic that we want to run.\n",
    "The `create_endpoint` call gives Serve a name for the endpoint, the backend that we want to use to serve requests, and the HTTP route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the backend. This will create an instance of ColorRecommender.\n",
    "client.create_backend(backend_tag=\"color:v1\", func_or_class=ColorRecommender)\n",
    "# Create an endpoint. This will route GET requests to /rec/color to the ColorRecommender backend.\n",
    "client.create_endpoint(endpoint_name=\"color\", backend=\"color:v1\", route=\"/rec/color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending requests\n",
    "\n",
    "Let's try sending a request to the new endpoint.\n",
    "You can also try this by visiting the URL directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import MOVIE_IDS\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_color_request(movie_id):\n",
    "    r = requests.get(\"http://localhost:8000/rec/color\", params={\"liked_id\": movie_id})\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        return r.json()\n",
    "    print(r.text)\n",
    "\n",
    "send_color_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also send a request directly to the endpoint using the Ray core API.\n",
    "First we use our Serve client to get a handle to the endpoint, using the endpoint name that we passed to the `create_endpoint` call.\n",
    "This will allow us to call the backend directly with a Ray *task*, which will return an `ObjectRef`, just like we did with Ray tasks in the first tutorial.\n",
    "One advantage of exposing methods this way is that we can now call an endpoint directly from Python, instead of having to go through HTTP first.\n",
    "\n",
    "Instead of defining the arguments in the HTTP request body, we'll pass them directly as keyword arguments to the remote function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_handle = client.get_handle(endpoint_name=\"color\")\n",
    "ray.get(color_handle.remote(liked_id=MOVIE_IDS[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create another endpoint\n",
    "\n",
    "There are lots of ways to provide movie recommendations!\n",
    "Let's create a second endpoint that provides recommendations based on the movie plot.\n",
    "We'll deploy a BERT NLP model that has been fine-tuned to determine similarity between movie plot descriptions.\n",
    "\n",
    "Here's a stateless version of the endpoint to get you started.\n",
    "The code loads plot vectors for each movie in our database that have been computed offline.\n",
    "Then, similar to the `ColorRecommender`, it finds the k-nearest neighbors of a movie liked by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from util import KNearestNeighborIndex\n",
    "\n",
    "def recommend_by_plot(request):\n",
    "    db = get_db_connection()\n",
    "\n",
    "    bert_vectors = db.execute(\n",
    "        \"SELECT id, plot_vector_json FROM movies\")\n",
    "    index = KNearestNeighborIndex(bert_vectors)\n",
    "\n",
    "    # Find k nearest movies with similar plots.\n",
    "    liked_id = request.args[\"liked_id\"]\n",
    "    num_returns = int(request.args.get(\"count\", 6))\n",
    "    recommended_movie_ids = index.search(liked_id, num_returns)\n",
    "\n",
    "    # Let's perform some post processing.\n",
    "    titles_and_ids = db.execute(\n",
    "        f\"SELECT title, id FROM movies WHERE id in ({','.join(recommended_movie_ids)})\"\n",
    "    ).fetchall()\n",
    "\n",
    "    # Wrangle the data for JSON\n",
    "    return [{\n",
    "        \"id\": movie_id,\n",
    "        \"title\": title\n",
    "    } for title, movie_id in titles_and_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just deploy this model as a stateless function, which Serve will invoke on each request.\n",
    "The problem with that is that we'll waste a lot of time loading the movie plot vectors on every request.\n",
    "To see that, let's try deploying a stateless backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_plot_request(movie_id):\n",
    "    r = requests.get(\"http://localhost:8000/rec/plot\", params={\"liked_id\": movie_id})\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        return r.json()\n",
    "    print(r.text)\n",
    "\n",
    "\n",
    "# Instantiate the backend. This is the same as the ColorRecommender,\n",
    "# except that we're deploying a stateless function.\n",
    "client.create_backend(backend_tag=\"plot:v1\", func_or_class=recommend_by_plot)\n",
    "# Create an endpoint. This will route GET requests to /rec/plot to the recommend_by_plot function.\n",
    "client.create_endpoint(endpoint_name=\"plot\", backend=\"plot:v1\", route=\"/rec/plot\")\n",
    "\n",
    "%timeit send_plot_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that again, but this time with a stateful backend!\n",
    "\n",
    "**Task:** Converting `PlotRecommender` to a stateful backend.\n",
    "1. Copy the code from the `recommend_by_plot` function to fill out the `PlotRecommender` class skeleton below. Make sure to load any state that should be reused between requests in the `__init__` method. \n",
    "2. Test it out by evaluating the following cell. You should see the same movie results as the `recommend_by_plot` backend, but the time per request should be much faster.\n",
    "\n",
    "**Tip:** Use the `ColorRecommender` structure as a reference.\n",
    "\n",
    "**If you haven't finished but want to move on:** We've included a reference implementation of `PlotRecommender` in the next cell. Show the code by clicking the \"...\" and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotRecommender:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, request):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PlotRecommender:\n",
    "    def __init__(self):\n",
    "        self.db = get_db_connection()\n",
    "\n",
    "        bert_vectors = self.db.execute(\n",
    "            \"SELECT id, plot_vector_json FROM movies\")\n",
    "        self.index = KNearestNeighborIndex(bert_vectors)\n",
    "\n",
    "    def __call__(self, request):\n",
    "        # Find k nearest movies with similar plots.\n",
    "        liked_id = request.args[\"liked_id\"]\n",
    "        num_returns = int(request.args.get(\"count\", 6))\n",
    "        recommended_movie_ids = self.index.search(liked_id, num_returns)\n",
    "\n",
    "        # Let's perform some post processing.\n",
    "        titles_and_ids = self.db.execute(\n",
    "            f\"SELECT title, id FROM movies WHERE id in ({','.join(recommended_movie_ids)})\"\n",
    "        ).fetchall()\n",
    "\n",
    "        # Wrangle the data for JSON\n",
    "        return [{\n",
    "            \"id\": movie_id,\n",
    "            \"title\": title\n",
    "        } for title, movie_id in titles_and_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the stateless backend.\n",
    "client.delete_endpoint(\"plot\")\n",
    "client.delete_backend(\"plot:v1\")\n",
    "\n",
    "# Instantiate the stateful backend.\n",
    "# Tip! You can run this cell again if you need to debug the PlotRecommender code.\n",
    "client.create_backend(backend_tag=\"plot:v1\", func_or_class=PlotRecommender)\n",
    "# Create an endpoint. This will route GET requests to /rec/plot to the recommend_by_plot function.\n",
    "client.create_endpoint(endpoint_name=\"plot\", backend=\"plot:v1\", route=\"/rec/plot\")\n",
    "\n",
    "%timeit send_plot_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use the Serve `client` to get a handle to the \"plot\" endpoint and compare the recommendations to the \"color\" endpoint.\n",
    "1. Get a handle to the \"plot\" endpoint. You can use the code to get the `color_handle` as a reference.\n",
    "2. Submit requests to the \"plot\" and \"color\" endpoints with the same \"liked_id\", and compare the returned recommendations. They should be completely different except for the movie that was passed as the \"liked_id\".\n",
    "> **Tip:** You can even do this in parallel! Try it by submitting all of the `.remote` functions first, then calling `ray.get` on a list of the results, like we did in notebook 1.\n",
    "\n",
    "Here's the code again for getting the results from the color endpoint, to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_handle = client.get_handle(endpoint_name=\"color\")\n",
    "ray.get(color_handle.remote(liked_id=MOVIE_IDS[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you've finished this notebook: head over to to [step 3](3.%20Deploying%20a%20custom%20ensemble%20model.ipynb), where we'll deploy a composed model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
