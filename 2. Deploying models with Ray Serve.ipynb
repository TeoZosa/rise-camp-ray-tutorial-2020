{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying your first model\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/master/serve/) is a library for scalable and programmable model serving.\n",
    "It aims to address some of the major challenges found in model serving:\n",
    "\n",
    "- **Framework-agnostic:** Model serving frameworks must be able to serve models from popular systems like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks.\n",
    "- **Supports application logic:** Machine learning models are typically surrounded by lots of application logic. In our application, this will come up when we decide which types of movie recommendations to serve to a particular user based on information about what that user has selected before.\n",
    "- **Python-first:** Configure your model serving with pure Python code - no more YAML or JSON configs.\n",
    "- **Simple and scalable:** Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "- **Flexible deployment patterns:** Ray Serve makes it easy to deploy a forest of models and to split traffic to different instances.\n",
    "\n",
    "See this [blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) and the [docs](https://docs.ray.io/en/master/serve/) for more background on Ray Serve!\n",
    "\n",
    "In this notebook, we'll deploy our first models using Ray Serve.\n",
    "We'll deploy one model that serves movie recommendations based on the movie cover's color palette and another that serves movie recommendations based on the movie's plot.\n",
    "\n",
    "## If you didn't finish notebook 1:\n",
    "\n",
    "Run the next cell to finish filling out the missing movie palettes in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Fill out the missing movie palettes in the database, in case you haven't finished notebook 1.\n",
    "bash run_1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can connect to the cluster with `ray.init`.\n",
    "This is the same as in the previous notebook: we'll pass in the argument `address=auto` to indicate that we should connect to an existing cluster that is running on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(address=\"auto\", ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll start Serve. This will set up an empty Flask server that can serve HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "\n",
    "try:\n",
    "    client = serve.start(detached=True)\n",
    "except:\n",
    "    # Skip if we already started Serve.\n",
    "    client = serve.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an endpoint\n",
    "\n",
    "Next, we'll define a *backend* for the server.\n",
    "Serve will send web traffic to this backend.\n",
    "\n",
    "Our first backend will be a stateful class that serves movie recommendations based on a movie cover's color palette.\n",
    "Each request to the backend will include the ID of a movie that the user liked (`liked_id`).\n",
    "The backend will use k-nearest neighbors to determine the movies closest to the user's selected movie.\n",
    "Since it would be expensive to have to reload the index on each request, we'll use a *stateful* backend to keep the index in memory between requests.\n",
    "\n",
    "We'll specify the logic that should get run on the server in the `__call__` method.\n",
    "Serve will pass each Flask request that gets routed to this backend as an argument to this method.\n",
    "That way, we can access any user arguments that are passed in the request, such as parameters in an HTTP `GET` request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_db_connection, KNearestNeighborIndex\n",
    "\n",
    "\n",
    "class ColorRecommender:\n",
    "    def __init__(self):\n",
    "        self.db = get_db_connection()\n",
    "\n",
    "        # Create index of cover image colors.\n",
    "        colors = self.db.execute(\"SELECT id, palette_json FROM movies\")\n",
    "        self.color_index = KNearestNeighborIndex(colors)\n",
    "\n",
    "    def __call__(self, request):\n",
    "        liked_id = request.args[\"liked_id\"]\n",
    "        num_returns = int(request.args.get(\"count\", 6))\n",
    "\n",
    "        # Perform KNN search for similar images.\n",
    "        recommended_ids = self.color_index.search(liked_id, num_returns)\n",
    "\n",
    "        # Let's perform some post processing.\n",
    "        titles_and_ids = self.db.execute(\n",
    "            f\"SELECT title, id FROM movies WHERE id in ({','.join(recommended_ids)})\"\n",
    "        ).fetchall()\n",
    "\n",
    "        # Wrangle the data for JSON\n",
    "        return [{\n",
    "            \"id\": movie_id,\n",
    "            \"title\": title\n",
    "        } for title, movie_id in titles_and_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define an *endpoint* and instantiate the backend.\n",
    "This will tell Serve which traffic should go to which backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the backend. This will create an instance of ColorRecommender.\n",
    "client.create_backend(\"color:v1\", ColorRecommender)\n",
    "# Create an endpoint. This will route GET requests to /rec/color to the ColorRecommender backend.\n",
    "client.create_endpoint(\"color\", backend=\"color:v1\", route=\"/rec/color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending requests\n",
    "\n",
    "Let's try sending a request to the new endpoint.\n",
    "You can also try this by visiting the URL directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import MOVIE_IDS\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def send_color_request(movie_id):\n",
    "    r = requests.get(\"http://localhost:8000/rec/color\", params={\"liked_id\": movie_id})\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        return r.json()\n",
    "    print(r.text)\n",
    "\n",
    "send_color_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also send a request directly to the backend using the Ray core API.\n",
    "First we use our Serve client to get a handle to the backend.\n",
    "This will allow us to call the remote `__call__` function that we defined and get an `ObjectRef`, just like we did with Ray tasks in the first tutorial.\n",
    "\n",
    "Instead of defining the arguments in the HTTP request body, we'll pass them directly as keyword arguments to the remote function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_handle = client.get_handle(\"color\")\n",
    "ray.get(color_handle.remote(liked_id=MOVIE_IDS[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create another endpoint\n",
    "\n",
    "There are lots of ways to provide movie recommendations!\n",
    "Let's create a second endpoint that provides recommendations based on the movie plot.\n",
    "We'll deploy a BERT NLP model that has been fine-tuned to determine similarity between movie plot descriptions.\n",
    "\n",
    "Here's a stateless version of the endpoint to get you started.\n",
    "The code loads plot vectors for each movie in our database that have been computed offline.\n",
    "Then, similar to the `ColorRecommender`, it finds the k-nearest neighbors of a movie liked by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from util import KNearestNeighborIndex\n",
    "\n",
    "def recommend_by_plot(request):\n",
    "    db = get_db_connection()\n",
    "\n",
    "    bert_vectors = db.execute(\n",
    "        \"SELECT id, plot_vector_json FROM movies\")\n",
    "    index = KNearestNeighborIndex(bert_vectors)\n",
    "\n",
    "    # Find k nearest movies with similar plots.\n",
    "    liked_id = request.args[\"liked_id\"]\n",
    "    num_returns = int(request.args.get(\"count\", 6))\n",
    "    recommended_movie_ids = index.search(liked_id, num_returns)\n",
    "\n",
    "    # Let's perform some post processing.\n",
    "    titles_and_ids = db.execute(\n",
    "        f\"SELECT title, id FROM movies WHERE id in ({','.join(recommended_movie_ids)})\"\n",
    "    ).fetchall()\n",
    "\n",
    "    # Wrangle the data for JSON\n",
    "    return [{\n",
    "        \"id\": movie_id,\n",
    "        \"title\": title\n",
    "    } for title, movie_id in titles_and_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just deploy this model as a stateless function, which Serve will invoke on each request.\n",
    "The problem with that is that we'll waste a lot of time loading the movie plot vectors on every request.\n",
    "To see that, let's try deploying a stateless backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_plot_request(movie_id):\n",
    "    r = requests.get(\"http://localhost:8000/rec/plot\", params={\"liked_id\": movie_id})\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        return r.json()\n",
    "    print(r.text)\n",
    "\n",
    "\n",
    "# Instantiate the backend. This is the same as the ColorRecommender,\n",
    "# except that we're deploying a stateless function.\n",
    "client.create_backend(\"plot:v1\", recommend_by_plot)\n",
    "# Create an endpoint. This will route GET requests to /rec/plot to the recommend_by_plot function.\n",
    "client.create_endpoint(\"plot\", backend=\"plot:v1\", route=\"/rec/plot\")\n",
    "\n",
    "%timeit send_plot_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that again, but this time with a stateful backend!\n",
    "\n",
    "**Task:** Converting `PlotRecommender` to a stateful backend.\n",
    "1. Copy the code from cell 22 to fill out the `PlotRecommender` class skeleton below. Make sure to load any state that should be reused between requests in the `__init__` method. \n",
    "2. Test it out by evaluating the following cell. You should see the same movie results as the `recommend_by_plot` backend, but the time per request should be much faster.\n",
    "\n",
    "**Tip:** Use the `ColorRecommender` structure as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotRecommender:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, request):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the stateless backend.\n",
    "client.delete_endpoint(\"plot\")\n",
    "client.delete_backend(\"plot:v1\")\n",
    "\n",
    "# Instantiate the stateful backend.\n",
    "# Tip! You can run this cell again if you need to debug the PlotRecommender code.\n",
    "client.create_backend(\"plot:v1\", PlotRecommender)\n",
    "# Create an endpoint. This will route GET requests to /rec/plot to the recommend_by_plot function.\n",
    "client.create_endpoint(\"plot\", backend=\"plot:v1\", route=\"/rec/plot\")\n",
    "\n",
    "%timeit send_plot_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Parametrize a Serve backend\n",
    "\n",
    "The backend that we just deployed uses the distances between movie plot vectors to determine which movies to return.\n",
    "We can also use another model to *rank* the movies returned.\n",
    "Let's try that now.\n",
    "\n",
    "We'll parametrize the backend to take in an optional ranking model.\n",
    "Here is some code showing how to use a pre-trained sklearn logistic regression model to sort a list of movie recommendations based on which is most likely to be selected next by the user.\n",
    "The `rank` method takes in a list of *movie IDs* and returns a list of movie IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import LRMovieRanker\n",
    "\n",
    "# Load the model weights.\n",
    "model_weights = get_db_connection().execute(\n",
    "    \"SELECT weights FROM models WHERE key='ranking/lr:base'\").fetchone()[0]\n",
    "base_lr_model = pickle.loads(model_weights)\n",
    "\n",
    "# Initialize the sklearn LogisticRegression classifier from the weights and the movie plot vectors.\n",
    "db = get_db_connection()\n",
    "bert_vectors = db.execute(\n",
    "    \"SELECT id, plot_vector_json FROM movies\")\n",
    "index = KNearestNeighborIndex(bert_vectors)\n",
    "lr_model = LRMovieRanker(base_lr_model, features=index.id_to_arr)\n",
    "\n",
    "# Try the classifier on the results returned by the PlotRecommender.\n",
    "movie_recs = send_plot_request(MOVIE_IDS[0])\n",
    "recommended_movie_ids = [rec['id'] for rec in movie_recs]\n",
    "print(\"unranked:\", recommended_movie_ids)\n",
    "ranked_ids = lr_model.rank_movies(recommended_movie_ids)\n",
    "print(\"ranked:\", ranked_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll parametrize the `PlotRecommender` class with an optional ranking model.\n",
    "\n",
    "**Task:** Implement and deploy a plot recommender that takes a ranking model in its constructor.\n",
    "1. Fill out the `RankedPlotRecommender` class below. The class takes in an optional `base_lr_model` as an argument to `__init__`.\n",
    "    - Modify the `__init__` method to initialize the `LRMovieRanker` using the `base_lr_model` (see cell 41 for reference).\n",
    "    - Modify the `__call__` method to execute the ranking function on the movie IDs returned by the k-nearest neighbors search.\n",
    "    \n",
    "    \n",
    "2. Check whether this worked by evaluating the following two cells. The first cell deletes the old `plot` endpoint and backend, then adds a new version of the backend and endpoint that instantiates the `RankedPlotRecommender` class.\n",
    "Serve will pass the `base_lr_model` argument to the `RankedPlotRecommender.__init__` method.\n",
    "The second cell sends a request to the new backend.\n",
    "The response should match the ranked movie order that we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankedPlotRecommender:\n",
    "    def __init__(self, base_lr_model):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, request):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the stateless backend.\n",
    "client.delete_endpoint(\"plot\")\n",
    "client.delete_backend(\"plot:v1\")\n",
    "\n",
    "# Instantiate the backend, parametrized with the base LR model.\n",
    "# Tip! You can run this cell again if you need to debug the RankedPlotRecommender code.\n",
    "client.create_backend(\"plot:v1\", RankedPlotRecommender, base_lr_model)\n",
    "# Create an endpoint. This will route GET requests to /rec/plot to the recommend_by_plot function.\n",
    "client.create_endpoint(\"plot\", backend=\"plot:v1\", route=\"/rec/plot\")\n",
    "\n",
    "%timeit send_plot_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_plot_request(MOVIE_IDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you've finished this notebook: head over to to [step 3](3.%20Deploying%20a%20custom%20ensemble%20model.ipynb), where we'll deploy a composed model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
